# v0.3.0 - Add Local LLM Support (Refined)

Enable the use of local LLMs (e.g., via LM Studio) and other providers by extracting LLM initialization into a dedicated module.

## User Review Required

> [!IMPORTANT]
> **New Module**: `apollo/apollo_1/llm.py` will handle LLM factory logic.
> **Environment Variables**:
> - `LLM_PROVIDER`: `openai`, `local`, `anthropic`
> - `LOCAL_LLM_BASE_URL`: URL for local LLM
> - `LOCAL_LLM_MODEL`: Model name
> - `ANTHROPIC_API_KEY`: Required if provider is `anthropic`

## Proposed Changes

### New Module

#### [NEW] [apollo/apollo_1/llm.py](file:///Volumes/Samsung970EVOPlus/dev-projects/apollo-1/apollo/apollo_1/llm.py)

- `get_llm(provider: str = None) -> BaseChatModel`: Factory function.
- Supports `openai` (default), `local` (LM Studio), `anthropic`.

### Agent Implementation

#### [MODIFY] [apollo/apollo_1/agents/srs_analyst.py](file:///Volumes/Samsung970EVOPlus/dev-projects/apollo-1/apollo/apollo_1/agents/srs_analyst.py)

- Replace direct `ChatOpenAI` instantiation with `from apollo.apollo_1.llm import get_llm`.
- Use `llm = get_llm()` to initialize.

### Fixes

#### [FIX] [Import Errors]

- Fix `AgentExecutor` import. It seems to be missing from `langchain.agents`. Will check `langchain.agents.agent` or `langchain.agents.executor`.

## Verification Plan

### Automated Tests
- Create `tests/test_llm.py` to test factory logic (mocking providers).
- Verify `srs_analyst` still instantiates correctly.

### Manual Verification
- Dry run with `LLM_PROVIDER=local` pointing to a mock server or LM Studio.
