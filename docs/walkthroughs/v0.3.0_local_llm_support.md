# v0.3.0 - Add Local LLM Support

Implemented configurable LLM provider support, allowing the use of Local LLMs (via LM Studio) and Anthropic alongside OpenAI.

## Changes

### New Features

#### LLM Factory (`apollo/apollo_1/llm.py`)
- Created `get_llm(provider=...)` factory function.
- Supports providers via `LLM_PROVIDER` env var:
  - `openai` (default)
  - `local` (e.g., LM Studio)
  - `anthropic` (Claude)

### Refactoring & Fixes

#### Agent Updates (`apollo/apollo_1/agents/srs_analyst.py`)
- **LLM Initialization**: Switched to `get_llm()` factory.
- **Import Fixes**:
  - `AgentExecutor`, `create_react_agent` → `langchain_classic.agents`
  - `hub` → `langchain_classic`
  - `CallbackHandler` → `langfuse.langchain`
- **Langfuse Fix**: Removed unsupported `secret_key` and `host` arguments from `CallbackHandler` init (now relies on env vars).

## Verification Results

### Automated Tests
Run `pytest` to verify all 21 tests pass:
```bash
source ./venv/bin/activate && pytest
```

Output:
```
tests/test_llm.py::TestLLMFactory::test_get_llm_openai_default PASSED
tests/test_llm.py::TestLLMFactory::test_get_llm_local PASSED
...
21 passed
```

### Manual Verification (Dry Run)

To use a local LLM (e.g., LM Studio):

```bash
export LLM_PROVIDER="local"
export LOCAL_LLM_BASE_URL="http://localhost:1234/v1"
# Run your workflow
```
